## Research Discussion 1
### Daniel DeBonis

## Metacritic and Cagematch
### 1. Choose one commercial recommender and describe how you think it works (content-based, collaborative filtering, etc). Does the technique deliver a good experience or are the recommendations off-target?
As the name suggests, Metacritic's mission is to provide an aggregate of the ratings of a curated group of critics into a single score for several types of media (video games, movies, television, and music). Therefore, this would be using collaborative filtering to form their model. The website mentions a "weighted average" of the scores of the critics. On the homepage, it is implied that the weight comes from the quality of the review, but they do not provide specifics. As such, the website does accurately capture what it claims. The purpose here is not to provide personalized recommendations but rather to capture critical consensus. 
The two primary questions prompted by the description are who is determined to be a critic worth integrating into the rating and how are the weights determined.The quality of the writing of the review seems to have a role in this decision, but again, this can be subjective. It seems the way that the website addresses this is by including a second score determined the average of the users of the website. This provides a comparison of the lay-person's opinion to that of the specified critics. Since it is completely dependent on the input of the users of the website, it is absolutely prone to the issues discussed in the second question.
### 2. Can you think of a similar example where a collective effort to alter the workings of content recommendations have been successful? How would you design a system to prevent this kind of abuse?
It shouldn't be too surprising that fans of professional wrestling can get wrapped up in rivalries to the point of overdramaticizing them. Cagematch is a website that calls itself the "Internet's Wrestling Database" since it tries to archive all of the matches that have taken place in wrestling promotions around the world, including a user-provided rating of the match's quality. Unsurprisingly, there had been coordinated efforts to throw off the ratings of the matches within certain promotions. The website mentions "Several methods are used to detect manipulation attempts" including those informed from other users of the website. This is likely a small enough community that manual detection might work for a significant percentage of the time, especially with volunteers trying to find such trends. Some obvious tells may include newly formed accounts or users giving away their biased rationale in a written review. The biggest change occurred about a year ago, when ratings for current events required written reviews for accounts created within a year, with the amount of time required to pass before leaving a rating increases with how newly the account was created. This was a clever compromise that has little impact on the older user base while adding in security against abuse from the newer users engaging in tribalistic score manipulation. 
In terms of what I would do to prevent, or at least minimize the impact of, this type of abuse, I would highlight the ratings that include reviews over those that do not, especially since written reviews require more effort and are easier to detect manipulation within. I would do something akin to what Metacritic does, and provide two averages, one for written reviews and another for all reviews (written or just provided a number rating).